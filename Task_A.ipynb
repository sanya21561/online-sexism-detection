{"cells":[{"cell_type":"code","source":["! wget https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n","! pip install transformers\n","! pip install optuna\n","! pip install gdown"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejorQYZsEPFi","executionInfo":{"status":"ok","timestamp":1714042325588,"user_tz":-330,"elapsed":48239,"user":{"displayName":"Prashasti Gupta","userId":"18285205696564661456"}},"outputId":"6fca804e-3b04-4d63-c539-bc83fbb3cdb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-04-25 10:51:16--  https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3846925 (3.7M) [text/plain]\n","Saving to: ‘edos_labelled_aggregated.csv’\n","\n","edos_labelled_aggre 100%[===================>]   3.67M  --.-KB/s    in 0.01s   \n","\n","2024-04-25 10:51:16 (253 MB/s) - ‘edos_labelled_aggregated.csv’ saved [3846925/3846925]\n","\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Collecting optuna\n","  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna)\n","  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","Installing collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.3 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"IJI3EAXmT8Oi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import pickle\n","import random\n","import gdown\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from copy import deepcopy\n","import matplotlib.pyplot as plt\n","import transformers\n","import torch.nn as nn\n","from collections import defaultdict, namedtuple, OrderedDict\n","from torch.utils.data import Dataset, DataLoader\n","from itertools import count\n","import torch.nn.functional as F\n","from torch.optim import AdamW, Adam\n","from sklearn.model_selection import train_test_split\n","from copy import deepcopy\n","from torch import Tensor\n","from sklearn.utils import shuffle\n","from typing import Union, Callable, Optional, Tuple\n","from transformers.optimization import get_linear_schedule_with_warmup\n","from transformers import BertModel, BertTokenizer, DebertaTokenizer, DebertaModel, RobertaTokenizer, RobertaModel\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WVH7aiDXFUA5","executionInfo":{"status":"ok","timestamp":1714042357338,"user_tz":-330,"elapsed":13301,"user":{"displayName":"Prashasti Gupta","userId":"18285205696564661456"}},"outputId":"983b50b9-55ba-4cac-95ff-78dfa29fc68f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["seeds = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n","seed_idx = 0\n","seed = seeds[seed_idx]\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)"],"metadata":{"id":"UWqn9QA9FW7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.read_csv('train_all_tasks.csv')\n","eval_label = pd.read_csv('dev_A.csv')\n","eval_text = pd.read_csv('dev_task_a_entries.csv')\n","eval_data = eval_text.merge(eval_label, on='rewire_id')\n","gab_unlabeled = pd.read_csv('gab_1M_unlabelled.csv')\n","reddit_unlabeled = pd.read_csv('reddit_1M_unlabelled.csv')\n","all_data_edos = pd.read_csv('edos_labelled_aggregated.csv')\n","test_data = deepcopy(all_data_edos[all_data_edos['split'] == 'test']).reset_index(drop=True)"],"metadata":{"id":"SDUbcGB6FbZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","deberta = 'microsoft/deberta-v3-large'\n","roberta = 'roberta-large'\n","bert = 'bert-large-uncased'\n","electra = 'google/electra-large-discriminator'\n","model_name = roberta\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name, output_hidden_states=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368,"referenced_widgets":["a38f597748744ce7815df674d2beb7e2","1a2c048bb63841158a907510ae7135a8","6627f13024194d9fb77e7835d27dbbdb","5aabd26e3c9d42fe929e304918c93f8e","c0ec0f2bcca243c89a05b0bc19810c60","689bef685b534fbaa87208e135695031","80b1b632f7df40c1b4157369bc008aef","c3795897954c47f28e7bb45211611209","9ceab1e51e1a4364a6063b3cf0fc64ba","a50972c0a0c54aeeb4cb28bcba313d33","93e767b76a1c444fa353b90616fd49da","c87938249ec34c06ac141946d1556a92","fee365ecd29f419b8142f17b0d1ff0f5","91146b2d11a644a7a096742fc0c00cd7","8bc8aaf90e9a4edf9e50a6949741b056","f1e3db639af745108c38c62689e140e2","aaa0521ca13049959b8ee1ee7b21b6ac","a5afc3fa873d45bea39aacda3f096026","f2c6829c26cf4a8d8dcae6560b963c57","11adebcc65cd43d5b8fd2326a2eded97","3167c7d5df9245989aabeea29306eb95","4e5ffee377294fe6b383844dc2df8150","b8dae964280646a79e86416e10df817a","c2c4d47b21394a6a880c4fdb17a103a5","ca187d7d68c64373a37dd4bcb4c27587","2d7ca311f3e240ee8a4cd434df0ae53f","08cfffa95be34594a53e82ec1f53f017","1a4ccb49dc834b619845a9e757bf30d6","3662834ef42449b8aef970b5bd301573","774eff078c644a9e8b712aa78f40a7d5","6fc9fd6e0a584f75a9af268ccd210883","fe5b809fb7164578aa2b54a92f74dbb4","ff7b747592b04310b0b56356025eaf4f","ec8714bab72641159a4d9e4dfd4827ae","c78175d7ed2b4c1f9080f96101ac5f91","0ea30abddcc74d94921c845ee91d9b91","40028368d8e44e928832775125016a9a","b8975fb9da394547847188cd29e10443","44fc1827e18949e0926c4f6c2b56fb78","34edb24fa63f42a7b0524a2e1a4662eb","ec7bbef2cfbe4f49a0b2604cdb0effde","0d474f178704401289a9eafbbc6ecd81","42816327280d4d1cb15a3d452e501099","f4b18bc09b5849c08606756daad91ad8","a89ca71a5940405b935b96768434326b","4a8e25537333427a96814a506fd66429","a49959f1958a494f954226a009ec519c","a355d1083e0b4e90a30e3c314e367a5c","0585db0ee8084d46b2a66db8b202e951","c67f8d19686d4fe59aa9782ca0dc2b2b","e289bfba147440248e500e8fefd64c0b","af7d357af54d46f1acd5eed69f5131ea","5f6e83c7efec41c5ac7ed20e55ead2b2","343ee34567d4473380ec8df01767c5c5","215bc36ccd144ebdb9f548d6bc412b0f","478d9382566b44d7abdc2aad10401794","6d9123ab016d4f23a047e9a9a573e8f5","e6d1479ea7b64b5f8c7475a67cc1a25f","cc858a48b5044259a4e4e977565cc501","9803be88c4da497f81bab8e599b43d83","a2891c9db1e34c42a43062da29a0fbb7","f6d94700413944b8be53b17144260dd2","8abe32c954f241fca4a7cb8b1622d749","119dbe46f3ce424a8311f59afa8f33d3","bbbf00ddeae149c789b205b9636ece21","5edd29a42e8c48a2b045a70745fb3ccc"]},"id":"ed5fJVHBFu5e","executionInfo":{"status":"ok","timestamp":1714042389800,"user_tz":-330,"elapsed":17465,"user":{"displayName":"Prashasti Gupta","userId":"18285205696564661456"}},"outputId":"dc72f12a-cf22-441b-9a5e-26e59a1c642a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38f597748744ce7815df674d2beb7e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87938249ec34c06ac141946d1556a92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8dae964280646a79e86416e10df817a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec8714bab72641159a4d9e4dfd4827ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89ca71a5940405b935b96768434326b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"478d9382566b44d7abdc2aad10401794"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def tolist(tensor):\n","  return tensor.detach().cpu().tolist()\n","\n","def map_names_2_ids(names):\n","  A = dict()\n","  for id, name in enumerate(names):\n","    A[name] = id\n","  return A\n","\n","def dist(x1, x2):\n","  return (x1 - x2).pow(2).sum(-1).sqrt()\n","\n","def entropy(logits):\n","  probs = F.softmax(logits, dim=-1)\n","  ent = -torch.sum((probs * torch.log2(probs)),dim=1)\n","  return ent\n","\n","def map_num_2_label(array, dict_map):\n","  new_dict_map = dict()\n","  new_array = list()\n","  for k, v in dict_map.items():\n","    new_dict_map[v] = k\n","  for element in array:\n","    new_array.append(new_dict_map[element])\n","  return new_array"],"metadata":{"id":"4agH5yKXFz1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_sexist = np.unique(train_data['label_sexist']).tolist()\n","label_category = np.unique(train_data['label_category']).tolist()\n","label_vector = np.unique(train_data['label_vector']).tolist()\n","label_category = [label_category[-1]] + label_category[:-1]\n","label_vector = [label_vector[-1]] + label_vector[:-1]\n","label_sexist_map = map_names_2_ids(label_sexist)\n","label_category_map = map_names_2_ids(label_category)\n","label_vector_map = map_names_2_ids(label_vector)\n","\n","train_data['Tag_A'] = [label_sexist_map[i[1]] for i in train_data['label_sexist'].items()]\n","eval_data['Tag_A'] = [label_sexist_map[i[1]] for i in eval_data['label'].items()]\n","train_data['Tag_B'] = [label_category_map[i[1]] for i in train_data['label_category'].items()]\n","train_data['Tag_C'] = [label_vector_map[i[1]] for i in train_data['label_vector'].items()]\n","test_data['Tag_A'] = [label_sexist_map[i[1]] for i in test_data['label_sexist'].items()]\n","labels_names = label_sexist\n","\n","train_dataframe = train_data\n","eval_dataframe = eval_data\n","test_dataframe = test_data\n","class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=train_dataframe['Tag_A'].values).tolist()"],"metadata":{"id":"2qc_VKeDF2gI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJIdNLZypaPc"},"outputs":[],"source":["class SexistDataset(Dataset):\n","\n","  def __init__(self, dataframe, tokenizer, max_length=100, use_label=True):\n","    self.dataframe = dataframe\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","    self.labels_names = f'{tokenizer.sep_token}'.join(labels_names)\n","    self.use_label = use_label\n","\n","    self.labels_tokens = []\n","    for label_name in labels_names:\n","      label_tokens = tokenizer(label_name, add_special_tokens=False)\n","      self.labels_tokens.append(label_tokens['input_ids'])\n","\n","  def __len__(self):\n","    return len(self.dataframe)\n","\n","  def __getitem__(self, idx):\n","    sample = self.dataframe.loc[idx]\n","    tokenized_text = tokenizer(\n","          sample['text'],\n","          max_length=self.max_length,\n","          padding='max_length',\n","          truncation='only_first',\n","          return_tensors='pt')\n","\n","    if self.use_label:\n","      labels_A = torch.LongTensor([sample['Tag_A']])\n","      tokenized_text['Tag_A'] = labels_A\n","\n","    return tokenized_text\n","\n","\n","class UnlabeledDataset:\n","  def __init__(self, dataframe, tokenizer, max_length=70, batch_size=10):\n","    self.data = dataframe\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","    self.batch_size = batch_size\n","    self.idxs = np.random.permutation(range(len(self.data)))\n","    self.current_idx = 0\n","    self.labels_names = f'{tokenizer.sep_token}'.join(labels_names)\n","    self.labels_tokens = []\n","    for label_name in labels_names:\n","      label_tokens = tokenizer(label_name, add_special_tokens=False)\n","      self.labels_tokens.append(label_tokens['input_ids'])\n","\n","  def __len__(self):\n","    return len(self.dataframe)\n","\n","  def combine_tenors(self, tensors_list):\n","    combined_tensor = dict()\n","    keys = tensors_list[0].keys()\n","    for key in keys:\n","      combined_tensor[key] = torch.cat([tensor[key] for tensor in tensors_list])\n","    return combined_tensor\n","\n","  def next(self):\n","    if self.current_idx >= len(self.data) - 1:\n","      self.current_idx = 0\n","    compact_data = self.data.iloc[self.idxs[self.current_idx: self.current_idx + self.batch_size]]\n","    self.current_idx += self.batch_size\n","    all_t = [self.tokenize(data) for _, data in compact_data.iterrows()]\n","    T = self.combine_tenors(all_t)\n","    return T\n","\n","  def tokenize(self, data):\n","    tokenized_text = self.tokenizer(data['text'], self.labels_names, padding='max_length', max_length=self.max_length, truncation=True, return_tensors='pt')\n","    input_ids = tokenized_text['input_ids']\n","    labels_start = (input_ids == tokenizer.sep_token_id).nonzero().contiguous().view(-1).tolist()[1] + 2\n","    labels_tokens_span = []\n","    c_token = labels_start\n","    for label_tokens in self.labels_tokens:\n","      labels_tokens_span.append([c_token, c_token + len(label_tokens) - 1])\n","      c_token += len(label_tokens) + 1\n","    tokenized_text['labels_tokens_span'] = torch.tensor(labels_tokens_span)\n","    return tokenized_text\n","\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, dim: int):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.sqrt_dim = np.sqrt(dim)\n","\n","    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n","        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n","\n","        if mask is not None:\n","            score.masked_fill_(mask.view(score.size()), -float('Inf'))\n","\n","        attn = F.softmax(score, -1)\n","        context = torch.bmm(attn, value)\n","        return context, attn\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model: int = 512, num_heads: int = 8):\n","        super(MultiHeadAttention, self).__init__()\n","\n","        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n","\n","        self.d_head = int(d_model / num_heads)\n","        self.num_heads = num_heads\n","        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n","        self.query_proj = nn.Linear(d_model, self.d_head * num_heads)\n","        self.key_proj = nn.Linear(d_model, self.d_head * num_heads)\n","        self.value_proj = nn.Linear(d_model, self.d_head * num_heads)\n","\n","    def forward(\n","            self,\n","            query: Tensor,\n","            key: Tensor,\n","            value: Tensor,\n","            mask: Optional[Tensor] = None\n","    ) -> Tuple[Tensor, Tensor]:\n","        batch_size = value.size(0)\n","\n","        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD\n","        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)      # BxK_LENxNxD\n","        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD\n","\n","        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD\n","        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD\n","        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD\n","\n","        if mask is not None:\n","            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # BxNxQ_LENxK_LEN\n","\n","        context, attn = self.scaled_dot_attn(query, key, value, mask)\n","\n","        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n","        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND\n","\n","        return context, attn\n","\n","class PGD():\n","\n","    def __init__(self, model,emb_name,epsilon=1.,alpha=0.3):\n","        # The emb_name parameter should be replaced with the parameter name of the embedding in your model\n","        self.model = model\n","        self.emb_name = emb_name\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.emb_backup = {}\n","        self.grad_backup = {}\n","\n","    # adversarial training : attack to change embedding abit with regards projected gradiant descent\n","    def attack(self,first_strike=False):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and self.emb_name in name:\n","                if first_strike:\n","                    # print('tt', param.data)\n","                    self.emb_backup[name] = param.data.clone()\n","                norm = torch.norm(param.grad)\n","                if norm != 0:\n","                    # Compute new params\n","                    r_at = self.alpha * param.grad / norm\n","                    param.data.add_(r_at)\n","                    param.data = self.project(name, param.data, self.epsilon)\n","\n","    # Restore to the back-up embeddings\n","    def restore(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and self.emb_name in name:\n","                assert name in self.emb_backup\n","                param.data = self.emb_backup[name]\n","        self.emb_backup = {}\n","\n","    # Project Gradiant Descent\n","    def project(self, param_name, param_data, epsilon):\n","        r = param_data - self.emb_backup[param_name]\n","        if torch.norm(r) > epsilon:\n","            r = epsilon * r / torch.norm(r)\n","        return self.emb_backup[param_name] + r\n","\n","    # Back-up parameters\n","    def backup_grad(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and 'pooler' not in name:\n","                self.grad_backup[name] = param.grad.clone()\n","\n","    # Restore grad parameters\n","    def restore_grad(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and 'pooler' not in name:\n","                param.grad = self.grad_backup[name]\n","\n","\n","def exists(value):\n","    return value is not None\n","\n","\n","def default(value, default):\n","    if exists(value):\n","        return value\n","    return default\n","\n","\n","def inf_norm(x):\n","    return torch.norm(x, p=float(\"inf\"), dim=-1, keepdim=True)\n","\n","\n","def kl_loss(input, target, reduction=\"batchmean\"):\n","    return F.kl_div(\n","        F.log_softmax(input, dim=-1),\n","        F.softmax(target, dim=-1),\n","        reduction=reduction,\n","    )\n","\n","\n","def sym_kl_loss(input, target, reduction=\"batchmean\", alpha=1.0):\n","    return alpha * F.kl_div(\n","        F.log_softmax(input, dim=-1),\n","        F.softmax(target.detach(), dim=-1),\n","        reduction=reduction,\n","    ) + F.kl_div(\n","        F.log_softmax(target, dim=-1),\n","        F.softmax(input.detach(), dim=-1),\n","        reduction=reduction,\n","    )\n","\n","\n","def js_loss(input, target, reduction=\"batchmean\", alpha=1.0):\n","    mean_proba = 0.5 * (\n","        F.softmax(input.detach(), dim=-1) + F.softmax(target.detach(), dim=-1)\n","    )\n","    return alpha * (\n","        F.kl_div(F.log_softmax(input, dim=-1), mean_proba, reduction=reduction)\n","        + F.kl_div(F.log_softmax(target, dim=-1), mean_proba, reduction=reduction)\n","    )\n","\n","\n","class SMARTLoss(nn.Module):\n","\n","    def __init__(\n","        self,\n","        model: nn.Module,\n","        loss_fn: Callable,\n","        loss_last_fn: Callable = None,\n","        norm_fn: Callable = inf_norm,\n","        num_steps: int = 1,\n","        step_size: float = 1e-3,\n","        epsilon: float = 1e-6,\n","        noise_var: float = 1e-5\n","    ) -> None:\n","        super().__init__()\n","        self.model = model\n","        self.loss_fn = loss_fn\n","        self.loss_last_fn = default(loss_last_fn, loss_fn)\n","        self.norm_fn = norm_fn\n","        self.num_steps = num_steps\n","        self.step_size = step_size\n","        self.epsilon = epsilon\n","        self.noise_var = noise_var\n","\n","    @torch.enable_grad()\n","    def forward(self, embed, state):\n","        noise = torch.randn_like(embed, requires_grad = True) * self.noise_var\n","\n","        # Indefinite loop with counter\n","        for i in count():\n","            # Compute perturbed embed and states\n","            embed_perturbed = embed + noise\n","            state_perturbed = self.model(embed_perturbed)\n","            # Return final loss if last step (undetached state)\n","            if i == self.num_steps:\n","                return self.loss_last_fn(state_perturbed, state)\n","            # Compute perturbation loss (detached state)\n","            loss = self.loss_fn(state_perturbed, state.detach())\n","            # Compute noise gradient ∂loss/∂noise\n","            noise_gradient, = torch.autograd.grad(loss, noise)\n","            # Move noise towards gradient to change state as much as possible\n","            step = noise + self.step_size * noise_gradient\n","            # Normalize new noise step into norm induced ball\n","            step_norm = self.norm_fn(step)\n","            noise = step / (step_norm + self.epsilon)\n","            # Reset noise gradients for next step\n","            noise = noise.detach().requires_grad_()\n","\n","\n","\n","\n","class ExtractedRoBERTa(nn.Module):\n","\n","    def __init__(self, model):\n","        super().__init__()\n","        self.roberta = model\n","        self.layers = model.encoder.layer\n","        self.attention_mask = None\n","        self.num_layers = len(self.layers) - 1\n","        self.head = nn.Linear(self.roberta.config.hidden_size, 2)\n","\n","    def forward(self, hidden, with_hidden_states = False, start_layer = 0, return_all=False):\n","        \"\"\" Forwards the hidden value from self.start_layer layer to the logits. \"\"\"\n","        hidden_states = []\n","\n","        for layer_id, layer in enumerate(self.layers[start_layer:]):\n","            hidden = layer(hidden, attention_mask = self.attention_mask)[0]\n","\n","            if layer_id in list(range(20, 24)):\n","              hidden_states += [hidden]\n","\n","        logits = self.head(hidden[:, 0, :])\n","\n","        if return_all:\n","          return logits, hidden_states\n","        else:\n","          return logits\n","\n","\n","    def get_embeddings(self, input_ids):\n","        \"\"\" Computes first embedding layer given inputs_ids \"\"\"\n","        return self.roberta.embeddings(input_ids)\n","\n","    def set_attention_mask(self, attention_mask):\n","        \"\"\" Sets the correct mask on all subsequent forward passes \"\"\"\n","        self.attention_mask = self.roberta.get_extended_attention_mask(\n","            attention_mask,\n","            input_shape = attention_mask.shape,\n","            # device = attention_mask.device\n","        ) # (b, 1, 1, s)\n","\n","\n","def ntxent(logits, labels, temp=.07):\n","  def ntx_loss(a, p, n, temp=temp):\n","    a = a.unsqueeze(0) if a.dim() == 1 else a\n","    p = p.unsqueeze(0) if p.dim() == 1 else p\n","    n = n.unsqueeze(0) if n.dim() == 1 else n\n","    assert a.dim() == 2\n","    assert p.dim() == 2\n","    assert n.dim() == 2\n","    a_p = a\n","    a_n = a.repeat(n.shape[0], 1)\n","    p_sim = F.cosine_similarity(a_p, p, dim=-1) / temp\n","    n_sim = F.cosine_similarity(a_n, n, dim=-1) / temp\n","\n","    # apply numeric stability\n","    max_val = torch.max(n_sim).detach()\n","    numerator = torch.exp(p_sim - max_val)\n","    denominator = torch.exp(n_sim - max_val).sum()\n","    loss = -torch.log(numerator / (denominator + numerator) + 1e-6)\n","    if loss.isnan():\n","      print(numerator, denominator)\n","      print(p_sim)\n","      print(len(n))\n","    # print(loss)\n","    return loss.mean()\n","\n","  def dist(x1, x2):\n","    return (x1 - x2).pow(2).sum(-1).sqrt()\n","\n","  con_losses = list()\n","  for i, (logit, label) in enumerate(zip(logits, labels)):\n","    ps = (labels == label)\n","    ns = (labels != label).nonzero().view(-1)\n","    ps[i] = False\n","    ps = ps.nonzero().view(-1)\n","    if len(ns):\n","      for p in ps:\n","        a_logit = logits[i]\n","        p_logit = logits[p]\n","        ns_logit = logits[ns]\n","        A = ntx_loss(a_logit, p_logit, ns_logit)\n","        con_losses.append(A)\n","\n","  if len(con_losses) > 0:\n","    all_con_loss = torch.stack(con_losses).mean()\n","  else:\n","    all_con_loss = torch.tensor(0.)\n","  return all_con_loss"]},{"cell_type":"code","source":["def train(dataloader, model, device, loss_fn, optimizer, scheduler, stage, ul_dataset, use_adv=True, use_vadv=True,\n","          use_ul=False, use_contrastive=True, con_weight=.5, vat_weight=.5, ul_weight=.5, adv_use_every_layer=True):\n","\n","  model.train()\n","  named_weights = [n for n, _ in model.named_parameters() if 'dense.weight' in n and 'pooler' not in n] + [\"word_embeddings.\"]\n","  loss_collection = [[], [], [], [], []]\n","  for step, data in enumerate(dataloader):\n","    if adv_use_every_layer:\n","      rand_layer = random.sample(named_weights, 1)[0]\n","      adv_layer = rand_layer\n","    else:\n","      adv_rand = random.uniform(0, 1)\n","      if adv_rand > .5:\n","        adv_layer = \"word_embeddings.\"\n","      else:\n","        rand_layer = random.sample(named_weights, 1)[0]\n","        adv_layer = rand_layer\n","    pgd = PGD(\n","      model=model,\n","      emb_name=adv_layer\n","    )\n","\n","    c_batch_size = data['input_ids'].shape[0]\n","    labels = data.pop('Tag_A').to(device).view(-1)\n","    for key in data:\n","      data[key] = data[key].to(device).view(c_batch_size, -1)\n","\n","    embeddings = model.get_embeddings(data['input_ids'].to(device))\n","    model.set_attention_mask(data['attention_mask'].to(device))\n","    logits = model(embeddings)\n","\n","    ce_loss = loss_fn(logits, labels)\n","    ce_loss.backward()\n","    loss_collection[0].append(ce_loss.item())\n","\n","\n","\n","    if use_adv:\n","      # PGD Start\n","        pgd.backup_grad()\n","        attack_times = 2\n","        for attack_time in range(attack_times):\n","            # Add adversarial perturbation to the embedding, backup param.data during the first attack\n","            pgd.attack(first_strike=(attack_time==0))\n","            if attack_time != attack_times-1:\n","              model.zero_grad()\n","            else:\n","              pgd.restore_grad()\n","\n","            embeddings = model.get_embeddings(data['input_ids'].to(device))\n","            model.set_attention_mask(data['attention_mask'].to(device))\n","            logits = model(embeddings)\n","            loss_adv = loss_fn(logits, labels)\n","            loss_adv.backward()\n","            loss_collection[1].append(loss_adv.item())\n","        pgd.restore()\n","\n","    if use_contrastive:\n","      embeddings = model.get_embeddings(data['input_ids'].to(device))\n","      model.set_attention_mask(data['attention_mask'].to(device))\n","      logits, hidden_states = model(embeddings, return_all=True)\n","      con_losses = []\n","      for hidden_idx, hidden_state in enumerate(hidden_states[::-1]):\n","        con_losses.append(ntxent(logits, labels) * con_weight * (1/(hidden_idx + 1)))\n","      con_loss = torch.stack(con_losses).mean()\n","      if con_loss.requires_grad:\n","        con_loss.backward()\n","      loss_collection[2].append(con_loss.item())\n","\n","    if use_vadv:\n","      vat_loss_fn = SMARTLoss(model = model, loss_fn = kl_loss, loss_last_fn = sym_kl_loss)\n","      # Compute VAT loss\n","      embeddings = model.get_embeddings(data['input_ids'].to(device))\n","      model.set_attention_mask(data['attention_mask'].to(device))\n","      logits = model(embeddings)\n","      vat_loss = vat_loss_fn(embeddings, logits)\n","      # Merge losses\n","      vat_loss = vat_weight * vat_loss\n","      vat_loss.backward()\n","      loss_collection[3].append(vat_loss.item())\n","\n","    if use_ul:\n","      ul_data = ul_dataset.next()\n","      c_batch_size = ul_data['input_ids'].shape[0]\n","      for key in ul_data:\n","        ul_data[key] = ul_data[key].to(device).view(c_batch_size, -1)\n","\n","      ul_embeddings = model.get_embeddings(ul_data['input_ids'].to(device))\n","      model.set_attention_mask(ul_data['attention_mask'].to(device))\n","      ul_logits = model(ul_embeddings)\n","\n","      vat_loss_fn = SMARTLoss(model = model, loss_fn = kl_loss, loss_last_fn = sym_kl_loss)\n","      vat_loss = vat_loss_fn(ul_embeddings, ul_logits)\n","      ul_loss = ul_weight * vat_loss\n","      loss_collection[4].append(ul_loss.item())\n","      ul_loss.backward()\n","\n","\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    scheduler.step()\n","\n","    if len(loss_collection[0]) % log_step == 0:\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | CE Loss {round(sum(loss_collection[0]) / (len(loss_collection[0]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | ADV Loss {round(sum(loss_collection[1]) / (len(loss_collection[1]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | CON Loss {round(sum(loss_collection[2]) / (len(loss_collection[2]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | VAT Loss {round(sum(loss_collection[3]) / (len(loss_collection[3]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | UL Loss {round(sum(loss_collection[4]) / (len(loss_collection[4]) + 1e-8), 4)}')\n","      print('------------------------------------------------')\n","      loss_collection = [[] for _ in range(5)]\n","\n","\n","def eval(dataloader, model, device):\n","  with torch.no_grad():\n","    model.eval()\n","    all_preds = list()\n","\n","    for data in dataloader:\n","      c_batch_size = data['input_ids'].shape[0]\n","      for key in data:\n","        data[key] = data[key].to(device).view(c_batch_size, -1)\n","      Tag_A = data.pop('Tag_A').to(device).view(-1)\n","      embeddings = model.get_embeddings(data['input_ids'].to(device))\n","      model.set_attention_mask(data['attention_mask'].to(device))\n","      logits = model(embeddings)\n","      preds = tolist(logits.argmax(1).view(-1))\n","      all_preds.extend(preds)\n","  return all_preds\n","\n","def test(dataloader, model, device):\n","  with torch.no_grad():\n","    model.eval()\n","    all_preds = list()\n","    for data in dataloader:\n","      c_batch_size = data['input_ids'].shape[0]\n","      for key in data:\n","        data[key] = data[key].to(device).view(c_batch_size, -1)\n","      embeddings = model.get_embeddings(data['input_ids'].to(device))\n","      model.set_attention_mask(data['attention_mask'].to(device))\n","      logits = model(embeddings)\n","      preds = tolist(logits.argmax(1).view(-1))\n","      all_preds.extend(preds)\n","  return all_preds"],"metadata":{"id":"8CF-CossGPmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = '.'\n","\n","filename = os.path.join(checkpoint_dir, 'best_ch.pt')\n","\n","def save_model(epoch, model, optimizer, scheduler, f1_list):\n","  torch.save(\n","      {'epoch': epoch,\n","       'model_state_dict': model.state_dict(),\n","       'optimizer_state_dict': optimizer.state_dict(),\n","       'scheduler_state_dict': scheduler.state_dict(),\n","       'f1_list': f1_list\n","       },\n","        filename)\n","\n","def load_model():\n","  if os.path.exists(filename):\n","    saved_dict = torch.load(filename)\n","    return True, saved_dict\n","  else:\n","    return False, None"],"metadata":{"id":"jU-GAk2LGUD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checkpoint_dir = '/content/drive/My Drive/'\n","\n","# filename = os.path.join(checkpoint_dir, 'TaskA_best_ch.pt')\n","\n","# def save_model(epoch, model, optimizer, scheduler, f1_list):\n","#   torch.save(\n","#       {'epoch': epoch,\n","#        'model_state_dict': model.state_dict(),\n","#        'optimizer_state_dict': optimizer.state_dict(),\n","#        'scheduler_state_dict': scheduler.state_dict(),\n","#        'f1_list': f1_list\n","#        },\n","#         filename)\n","\n","# def load_model():\n","#   if os.path.exists(filename):\n","#     saved_dict = torch.load(filename)\n","#     return True, saved_dict\n","#   else:\n","#     return False, None"],"metadata":{"id":"x1BigqmVUFNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","lr = 4e-6\n","beta_1 = .9\n","beta_2 = .999\n","eps = 1e-6\n","log_step = 100\n","batch_size = 10\n","weight_decay = 1e-2\n","max_length = 70\n","\n","vat_weight = .5\n","ul_weight = .5\n","ent_weight = .5\n","label_smoothing = .0\n","\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","# sexist_model = SexistModel(deepcopy(model)).to(device)\n","sexist_model = ExtractedRoBERTa(deepcopy(model)).to(device)\n","loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device), label_smoothing=label_smoothing).to(device)\n","loss_collection = []\n","\n","train_dataset = SexistDataset(train_dataframe, tokenizer, max_length)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","eval_dataset = SexistDataset(eval_dataframe, tokenizer, max_length)\n","eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n","\n","test_dataset = SexistDataset(test_dataframe, tokenizer, max_length)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","optimization_steps = epochs * len(train_dataloader)\n","warmup_ratio = .0\n","warmup_steps = int(optimization_steps * warmup_ratio)\n","\n","d = reddit_unlabeled.sample(frac=.1).reset_index(drop=True)\n","un = UnlabeledDataset(d, tokenizer)\n","\n","optimizer = AdamW(sexist_model.parameters(), lr=lr, betas=(beta_1,beta_2), eps=eps, weight_decay=weight_decay)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=warmup_steps,\n","    num_training_steps=optimization_steps)\n","\n","best_f1 = 0.\n","all_f1 = list()\n","best_model = None\n","best_preds = None\n","transformers.logging.set_verbosity_error()\n","start_epoch = 0\n","\n","checkpoint_avl, saved_dict = load_model()\n","print(f'Checkpoint is {checkpoint_avl}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GHUsJtTrGJ-q","executionInfo":{"status":"ok","timestamp":1714042393356,"user_tz":-330,"elapsed":3799,"user":{"displayName":"Prashasti Gupta","userId":"18285205696564661456"}},"outputId":"2eb93453-3c40-48da-d4f2-d988e2d588fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint is False\n"]}]},{"cell_type":"code","source":["checkpoint_avl, saved_dict = load_model()\n","print(f'Checkpoint is {checkpoint_avl}')\n","\n","if checkpoint_avl:\n","  start_epoch = saved_dict['epoch']\n","  model_state_dict = saved_dict['model_state_dict']\n","  optimizer_state_dict = saved_dict['optimizer_state_dict']\n","  scheduler_state_dict = saved_dict['scheduler_state_dict']\n","  all_f1 = saved_dict['f1_list']\n","  best_f1 = max(all_f1)\n","\n","  sexist_model.load_state_dict(model_state_dict)\n","  optimizer.load_state_dict(optimizer_state_dict)\n","  scheduler.load_state_dict(scheduler_state_dict)\n","  print('model loaded')"],"metadata":{"id":"EeGqrC90__-x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714042393358,"user_tz":-330,"elapsed":200,"user":{"displayName":"Prashasti Gupta","userId":"18285205696564661456"}},"outputId":"a81666c0-93cb-4e35-e38d-a45263a01dc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint is False\n"]}]},{"cell_type":"code","source":["for epoch in range(start_epoch, epochs):\n","  train(train_dataloader, sexist_model, device, loss_fn, optimizer, scheduler, 1, None)\n","  preds_A_eval = eval(eval_dataloader, sexist_model, device)\n","  f1_macro_A_eval = f1_score(eval_dataframe['Tag_A'].values.tolist(), preds_A_eval, average='macro')\n","  all_f1.append(f1_macro_A_eval)\n","  if f1_macro_A_eval > best_f1:\n","    best_f1 = f1_macro_A_eval\n","    best_preds = preds_A_eval\n","    save_model(epoch + 1, sexist_model, optimizer, scheduler, all_f1)\n","\n","  tw = f'EPOCH [{epoch + 1}/{epochs}] | Current F1-Macro {round(f1_macro_A_eval * 100, 2)}\\n'\n","  with open(f'pred.txt', 'a') as f:\n","    f.write(tw)\n","  print(f'EPOCH [{epoch + 1}/{epochs}] | Current F1-Macro {round(f1_macro_A_eval * 100, 2)}')\n","  print(f'EPOCH [{epoch + 1}/{epochs}] | Best F1-Macro {round(best_f1 * 100, 2)}')\n","\n","with open(f'pred_file_{seed}.pk', 'wb') as f:\n","  pickle.dump(best_preds, f)"],"metadata":{"id":"ZT6ce-ZM_snA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f33806d2-aacd-43bf-8147-728dcc67746c","executionInfo":{"status":"ok","timestamp":1714067833784,"user_tz":-330,"elapsed":6,"user":{"displayName":"Parisha Agrawal","userId":"18422913939848700564"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Checkpoit is False\n","model loaded\n","EPOCH [1/3] | STEP [100/1400] | CE Loss 0.7049\n","EPOCH [1/3] | STEP [100/1400] | ADV Loss 0.7565\n","EPOCH [1/3] | STEP [100/1400] | CON Loss 0.2011\n","EPOCH [1/3] | STEP [100/1400] | VAT Loss 0.0075\n","EPOCH [1/3] | STEP [100/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [200/1400] | CE Loss 0.6999\n","EPOCH [1/3] | STEP [200/1400] | ADV Loss 0.7323\n","EPOCH [1/3] | STEP [200/1400] | CON Loss 0.1723\n","EPOCH [1/3] | STEP [200/1400] | VAT Loss 0.0192\n","EPOCH [1/3] | STEP [200/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [300/1400] | CE Loss 0.6839\n","EPOCH [1/3] | STEP [300/1400] | ADV Loss 0.7498\n","EPOCH [1/3] | STEP [300/1400] | CON Loss 0.1835\n","EPOCH [1/3] | STEP [300/1400] | VAT Loss 0.0137\n","EPOCH [1/3] | STEP [300/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [400/1400] | CE Loss 0.6313\n","EPOCH [1/3] | STEP [400/1400] | ADV Loss 0.6945\n","EPOCH [1/3] | STEP [400/1400] | CON Loss 0.1778\n","EPOCH [1/3] | STEP [400/1400] | VAT Loss 0.0254\n","EPOCH [1/3] | STEP [400/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [500/1400] | CE Loss 0.5609\n","EPOCH [1/3] | STEP [500/1400] | ADV Loss 0.6673\n","EPOCH [1/3] | STEP [500/1400] | CON Loss 0.179\n","EPOCH [1/3] | STEP [500/1400] | VAT Loss 0.0383\n","EPOCH [1/3] | STEP [500/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [600/1400] | CE Loss 0.5145\n","EPOCH [1/3] | STEP [600/1400] | ADV Loss 0.6215\n","EPOCH [1/3] | STEP [600/1400] | CON Loss 0.1502\n","EPOCH [1/3] | STEP [600/1400] | VAT Loss 0.0497\n","EPOCH [1/3] | STEP [600/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [700/1400] | CE Loss 0.4833\n","EPOCH [1/3] | STEP [700/1400] | ADV Loss 0.5889\n","EPOCH [1/3] | STEP [700/1400] | CON Loss 0.138\n","EPOCH [1/3] | STEP [700/1400] | VAT Loss 0.0426\n","EPOCH [1/3] | STEP [700/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [800/1400] | CE Loss 0.4453\n","EPOCH [1/3] | STEP [800/1400] | ADV Loss 0.572\n","EPOCH [1/3] | STEP [800/1400] | CON Loss 0.1237\n","EPOCH [1/3] | STEP [800/1400] | VAT Loss 0.0436\n","EPOCH [1/3] | STEP [800/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [900/1400] | CE Loss 0.4422\n","EPOCH [1/3] | STEP [900/1400] | ADV Loss 0.5683\n","EPOCH [1/3] | STEP [900/1400] | CON Loss 0.1303\n","EPOCH [1/3] | STEP [900/1400] | VAT Loss 0.0318\n","EPOCH [1/3] | STEP [900/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [1000/1400] | CE Loss 0.4178\n","EPOCH [1/3] | STEP [1000/1400] | ADV Loss 0.5546\n","EPOCH [1/3] | STEP [1000/1400] | CON Loss 0.1346\n","EPOCH [1/3] | STEP [1000/1400] | VAT Loss 0.0296\n","EPOCH [1/3] | STEP [1000/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [1100/1400] | CE Loss 0.4077\n","EPOCH [1/3] | STEP [1100/1400] | ADV Loss 0.5242\n","EPOCH [1/3] | STEP [1100/1400] | CON Loss 0.1159\n","EPOCH [1/3] | STEP [1100/1400] | VAT Loss 0.0307\n","EPOCH [1/3] | STEP [1100/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [1200/1400] | CE Loss 0.4226\n","EPOCH [1/3] | STEP [1200/1400] | ADV Loss 0.53\n","EPOCH [1/3] | STEP [1200/1400] | CON Loss 0.1311\n","EPOCH [1/3] | STEP [1200/1400] | VAT Loss 0.0262\n","EPOCH [1/3] | STEP [1200/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [1300/1400] | CE Loss 0.4057\n","EPOCH [1/3] | STEP [1300/1400] | ADV Loss 0.5222\n","EPOCH [1/3] | STEP [1300/1400] | CON Loss 0.1272\n","EPOCH [1/3] | STEP [1300/1400] | VAT Loss 0.0249\n","EPOCH [1/3] | STEP [1300/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | STEP [1400/1400] | CE Loss 0.4047\n","EPOCH [1/3] | STEP [1400/1400] | ADV Loss 0.5246\n","EPOCH [1/3] | STEP [1400/1400] | CON Loss 0.1329\n","EPOCH [1/3] | STEP [1400/1400] | VAT Loss 0.0273\n","EPOCH [1/3] | STEP [1400/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/3] | Current F1-Macro 81.37\n","EPOCH [1/3] | Best F1-Macro 81.37\n","EPOCH [2/3] | STEP [100/1400] | CE Loss 0.3953\n","EPOCH [2/3] | STEP [100/1400] | ADV Loss 0.5176\n","EPOCH [2/3] | STEP [100/1400] | CON Loss 0.1166\n","EPOCH [2/3] | STEP [100/1400] | VAT Loss 0.0291\n","EPOCH [2/3] | STEP [100/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [200/1400] | CE Loss 0.3486\n","EPOCH [2/3] | STEP [200/1400] | ADV Loss 0.4779\n","EPOCH [2/3] | STEP [200/1400] | CON Loss 0.1186\n","EPOCH [2/3] | STEP [200/1400] | VAT Loss 0.0378\n","EPOCH [2/3] | STEP [200/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [300/1400] | CE Loss 0.3711\n","EPOCH [2/3] | STEP [300/1400] | ADV Loss 0.5036\n","EPOCH [2/3] | STEP [300/1400] | CON Loss 0.1228\n","EPOCH [2/3] | STEP [300/1400] | VAT Loss 0.0387\n","EPOCH [2/3] | STEP [300/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [400/1400] | CE Loss 0.3734\n","EPOCH [2/3] | STEP [400/1400] | ADV Loss 0.5197\n","EPOCH [2/3] | STEP [400/1400] | CON Loss 0.1173\n","EPOCH [2/3] | STEP [400/1400] | VAT Loss 0.03\n","EPOCH [2/3] | STEP [400/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [500/1400] | CE Loss 0.3217\n","EPOCH [2/3] | STEP [500/1400] | ADV Loss 0.4475\n","EPOCH [2/3] | STEP [500/1400] | CON Loss 0.0991\n","EPOCH [2/3] | STEP [500/1400] | VAT Loss 0.0309\n","EPOCH [2/3] | STEP [500/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [600/1400] | CE Loss 0.3227\n","EPOCH [2/3] | STEP [600/1400] | ADV Loss 0.4395\n","EPOCH [2/3] | STEP [600/1400] | CON Loss 0.0999\n","EPOCH [2/3] | STEP [600/1400] | VAT Loss 0.0271\n","EPOCH [2/3] | STEP [600/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [700/1400] | CE Loss 0.3474\n","EPOCH [2/3] | STEP [700/1400] | ADV Loss 0.4783\n","EPOCH [2/3] | STEP [700/1400] | CON Loss 0.1259\n","EPOCH [2/3] | STEP [700/1400] | VAT Loss 0.0226\n","EPOCH [2/3] | STEP [700/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [800/1400] | CE Loss 0.3374\n","EPOCH [2/3] | STEP [800/1400] | ADV Loss 0.4856\n","EPOCH [2/3] | STEP [800/1400] | CON Loss 0.1145\n","EPOCH [2/3] | STEP [800/1400] | VAT Loss 0.0253\n","EPOCH [2/3] | STEP [800/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [900/1400] | CE Loss 0.3506\n","EPOCH [2/3] | STEP [900/1400] | ADV Loss 0.4924\n","EPOCH [2/3] | STEP [900/1400] | CON Loss 0.1078\n","EPOCH [2/3] | STEP [900/1400] | VAT Loss 0.0227\n","EPOCH [2/3] | STEP [900/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [1000/1400] | CE Loss 0.3287\n","EPOCH [2/3] | STEP [1000/1400] | ADV Loss 0.4728\n","EPOCH [2/3] | STEP [1000/1400] | CON Loss 0.102\n","EPOCH [2/3] | STEP [1000/1400] | VAT Loss 0.0166\n","EPOCH [2/3] | STEP [1000/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [1100/1400] | CE Loss 0.3287\n","EPOCH [2/3] | STEP [1100/1400] | ADV Loss 0.4624\n","EPOCH [2/3] | STEP [1100/1400] | CON Loss 0.1214\n","EPOCH [2/3] | STEP [1100/1400] | VAT Loss 0.0231\n","EPOCH [2/3] | STEP [1100/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [1200/1400] | CE Loss 0.3363\n","EPOCH [2/3] | STEP [1200/1400] | ADV Loss 0.4689\n","EPOCH [2/3] | STEP [1200/1400] | CON Loss 0.1093\n","EPOCH [2/3] | STEP [1200/1400] | VAT Loss 0.0211\n","EPOCH [2/3] | STEP [1200/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [1300/1400] | CE Loss 0.3509\n","EPOCH [2/3] | STEP [1300/1400] | ADV Loss 0.4738\n","EPOCH [2/3] | STEP [1300/1400] | CON Loss 0.1227\n","EPOCH [2/3] | STEP [1300/1400] | VAT Loss 0.026\n","EPOCH [2/3] | STEP [1300/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | STEP [1400/1400] | CE Loss 0.3589\n","EPOCH [2/3] | STEP [1400/1400] | ADV Loss 0.464\n","EPOCH [2/3] | STEP [1400/1400] | CON Loss 0.115\n","EPOCH [2/3] | STEP [1400/1400] | VAT Loss 0.0299\n","EPOCH [2/3] | STEP [1400/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/3] | Current F1-Macro 83.49\n","EPOCH [2/3] | Best F1-Macro 83.49\n","EPOCH [3/3] | STEP [100/1400] | CE Loss 0.308\n","EPOCH [3/3] | STEP [100/1400] | ADV Loss 0.399\n","EPOCH [3/3] | STEP [100/1400] | CON Loss 0.101\n","EPOCH [3/3] | STEP [100/1400] | VAT Loss 0.0317\n","EPOCH [3/3] | STEP [100/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [200/1400] | CE Loss 0.2898\n","EPOCH [3/3] | STEP [200/1400] | ADV Loss 0.4355\n","EPOCH [3/3] | STEP [200/1400] | CON Loss 0.098\n","EPOCH [3/3] | STEP [200/1400] | VAT Loss 0.0398\n","EPOCH [3/3] | STEP [200/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [300/1400] | CE Loss 0.2983\n","EPOCH [3/3] | STEP [300/1400] | ADV Loss 0.4407\n","EPOCH [3/3] | STEP [300/1400] | CON Loss 0.0998\n","EPOCH [3/3] | STEP [300/1400] | VAT Loss 0.027\n","EPOCH [3/3] | STEP [300/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [400/1400] | CE Loss 0.2763\n","EPOCH [3/3] | STEP [400/1400] | ADV Loss 0.4298\n","EPOCH [3/3] | STEP [400/1400] | CON Loss 0.0891\n","EPOCH [3/3] | STEP [400/1400] | VAT Loss 0.0261\n","EPOCH [3/3] | STEP [400/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [500/1400] | CE Loss 0.3103\n","EPOCH [3/3] | STEP [500/1400] | ADV Loss 0.444\n","EPOCH [3/3] | STEP [500/1400] | CON Loss 0.1036\n","EPOCH [3/3] | STEP [500/1400] | VAT Loss 0.0221\n","EPOCH [3/3] | STEP [500/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [600/1400] | CE Loss 0.2847\n","EPOCH [3/3] | STEP [600/1400] | ADV Loss 0.4299\n","EPOCH [3/3] | STEP [600/1400] | CON Loss 0.0892\n","EPOCH [3/3] | STEP [600/1400] | VAT Loss 0.0338\n","EPOCH [3/3] | STEP [600/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [700/1400] | CE Loss 0.2792\n","EPOCH [3/3] | STEP [700/1400] | ADV Loss 0.4032\n","EPOCH [3/3] | STEP [700/1400] | CON Loss 0.0869\n","EPOCH [3/3] | STEP [700/1400] | VAT Loss 0.0249\n","EPOCH [3/3] | STEP [700/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [800/1400] | CE Loss 0.2649\n","EPOCH [3/3] | STEP [800/1400] | ADV Loss 0.4049\n","EPOCH [3/3] | STEP [800/1400] | CON Loss 0.0901\n","EPOCH [3/3] | STEP [800/1400] | VAT Loss 0.0295\n","EPOCH [3/3] | STEP [800/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [900/1400] | CE Loss 0.2566\n","EPOCH [3/3] | STEP [900/1400] | ADV Loss 0.4115\n","EPOCH [3/3] | STEP [900/1400] | CON Loss 0.0794\n","EPOCH [3/3] | STEP [900/1400] | VAT Loss 0.0295\n","EPOCH [3/3] | STEP [900/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [1000/1400] | CE Loss 0.2786\n","EPOCH [3/3] | STEP [1000/1400] | ADV Loss 0.4141\n","EPOCH [3/3] | STEP [1000/1400] | CON Loss 0.0894\n","EPOCH [3/3] | STEP [1000/1400] | VAT Loss 0.0348\n","EPOCH [3/3] | STEP [1000/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [1100/1400] | CE Loss 0.2974\n","EPOCH [3/3] | STEP [1100/1400] | ADV Loss 0.4277\n","EPOCH [3/3] | STEP [1100/1400] | CON Loss 0.0955\n","EPOCH [3/3] | STEP [1100/1400] | VAT Loss 0.0327\n","EPOCH [3/3] | STEP [1100/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [1200/1400] | CE Loss 0.2835\n","EPOCH [3/3] | STEP [1200/1400] | ADV Loss 0.4016\n","EPOCH [3/3] | STEP [1200/1400] | CON Loss 0.0829\n","EPOCH [3/3] | STEP [1200/1400] | VAT Loss 0.0231\n","EPOCH [3/3] | STEP [1200/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [1300/1400] | CE Loss 0.2639\n","EPOCH [3/3] | STEP [1300/1400] | ADV Loss 0.4049\n","EPOCH [3/3] | STEP [1300/1400] | CON Loss 0.0901\n","EPOCH [3/3] | STEP [1300/1400] | VAT Loss 0.0295\n","EPOCH [3/3] | STEP [1300/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | STEP [1400/1400] | CE Loss 0.2566\n","EPOCH [3/3] | STEP [1400/1400] | ADV Loss 0.4115\n","EPOCH [3/3] | STEP [1400/1400] | CON Loss 0.0794\n","EPOCH [3/3] | STEP [1400/1400] | VAT Loss 0.0295\n","EPOCH [3/3] | STEP [1400/1400] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/3] | Current F1-Macro 84.51\n","EPOCH [3/3] | Best F1-Macro 84.51\n"]}]},{"cell_type":"code","source":["checkpoint_avl, saved_dict = load_model()\n","print(f'Checkpoint is {checkpoint_avl}')\n","\n","if checkpoint_avl:\n","  start_epoch = saved_dict['epoch']\n","  model_state_dict = saved_dict['model_state_dict']\n","  optimizer_state_dict = saved_dict['optimizer_state_dict']\n","  scheduler_state_dict = saved_dict['scheduler_state_dict']\n","  all_f1 = saved_dict['f1_list']\n","  best_f1 = max(all_f1)\n","\n","  sexist_model.load_state_dict(model_state_dict)\n","  optimizer.load_state_dict(optimizer_state_dict)\n","  scheduler.load_state_dict(scheduler_state_dict)\n","  print('model loaded')"],"metadata":{"id":"LZNA_26TAQXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, saved_dict = load_model()\n","sexist_model.load_state_dict(saved_dict['model_state_dict'])\n","\n","preds_A_test = eval(test_dataloader, sexist_model, device)\n","f1_macro_A_test = f1_score(test_dataframe['Tag_A'].values.tolist(), preds_A_test, average='macro')\n","\n","tw = f'Test F1-Macro {round(f1_macro_A_test * 100, 2)}\\n'\n","with open(f'test_result.txt', 'a') as f:\n","  f.write(tw)"],"metadata":{"id":"XhvX7tdgGcT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxcN0uW1sCkI"},"outputs":[],"source":["preds_A_test = eval(test_dataloader, sexist_model, device)\n","f1_macro_A_test = f1_score(test_dataframe['Tag_A'].values.tolist(), preds_A_test, average='macro')\n","pr_A_test = precision_score(test_dataframe['Tag_A'].values.tolist(), preds_A_test, average='macro')\n","rc_A_test = recall_score(test_dataframe['Tag_A'].values.tolist(), preds_A_test, average='macro')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEljscvZvlSt","outputId":"c34b7ad0-8982-497c-8ac4-8a694d35a2b6","executionInfo":{"status":"ok","timestamp":1714058797629,"user_tz":-330,"elapsed":423,"user":{"displayName":"Parisha Agrawal","userId":"18422913939848700564"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.812870960965139\n"]}],"source":["pr_A_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGZk7VQWvoDQ","outputId":"88187461-66c3-42be-ce93-ca2832f96167","executionInfo":{"status":"ok","timestamp":1714058845169,"user_tz":-330,"elapsed":951,"user":{"displayName":"Parisha Agrawal","userId":"18422913939848700564"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.8137689904346577\n"]}],"source":["rc_A_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0wTOJstsNIj","outputId":"75f6d8dc-0782-4ac9-bf50-1e85db2d97a0","executionInfo":{"status":"ok","timestamp":1714059070318,"user_tz":-330,"elapsed":530,"user":{"displayName":"Parisha Agrawal","userId":"18422913939848700564"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.8133197278094763\n"]}],"source":["f1_macro_A_test"]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset\n","import torch\n","\n","# Define a simple dataset containing just one sentence\n","class SingleSentenceDataset(Dataset):\n","    def __init__(self, sentence, tokenizer, max_length):\n","        self.sentence = sentence\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return 1\n","\n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(self.sentence, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n","        return {\n","            'input_ids': encoding['input_ids'].view(-1),\n","            'attention_mask': encoding['attention_mask'].view(-1)\n","        }"],"metadata":{"id":"F6BANxtP4ZXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence_to_test = \"O come on there's no way any men are attracted to her shes a pig🙄\"\n","single_sentence_dataset = SingleSentenceDataset(sentence_to_test, tokenizer, max_length)\n","single_sentence_dataloader = DataLoader(single_sentence_dataset, batch_size=1)\n","test_predictions = test(single_sentence_dataloader, sexist_model, device)\n","# print(test_predictions)\n","prediction_label = \"sexist\" if test_predictions[0] == 1 else \"not sexist\"\n","print(f\"{prediction_label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_9mtwVcxUOnE","executionInfo":{"status":"ok","timestamp":1714066846313,"user_tz":-330,"elapsed":9,"user":{"displayName":"Parisha Agrawal","userId":"18422913939848700564"}},"outputId":"77650777-cd26-43d2-d069-cb4bfedbee74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sexist\n"]}]},{"cell_type":"code","source":["sentence_to_test = \"O come on there's no way any men are attracted to her shes a [MASK]\"\n","single_sentence_dataset = SingleSentenceDataset(sentence_to_test, tokenizer, max_length)\n","single_sentence_dataloader = DataLoader(single_sentence_dataset, batch_size=1)\n","test_predictions = test(single_sentence_dataloader, sexist_model, device)\n","# print(test_predictions)\n","prediction_label = \"sexist\" if test_predictions[0] == 1 else \"not sexist\"\n","print(f\"{prediction_label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssrfxjYNV-dV","executionInfo":{"status":"ok","timestamp":1714066846314,"user_tz":-330,"elapsed":7,"user":{"displayName":"Parisha Agrawal","userId":"18422913939848700564"}},"outputId":"d69ec362-a9c9-4164-fc3c-08309ff1891c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sexist\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RyC4SN23WEUl"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a38f597748744ce7815df674d2beb7e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a2c048bb63841158a907510ae7135a8","IPY_MODEL_6627f13024194d9fb77e7835d27dbbdb","IPY_MODEL_5aabd26e3c9d42fe929e304918c93f8e"],"layout":"IPY_MODEL_c0ec0f2bcca243c89a05b0bc19810c60"}},"1a2c048bb63841158a907510ae7135a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_689bef685b534fbaa87208e135695031","placeholder":"​","style":"IPY_MODEL_80b1b632f7df40c1b4157369bc008aef","value":"tokenizer_config.json: 100%"}},"6627f13024194d9fb77e7835d27dbbdb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3795897954c47f28e7bb45211611209","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ceab1e51e1a4364a6063b3cf0fc64ba","value":25}},"5aabd26e3c9d42fe929e304918c93f8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a50972c0a0c54aeeb4cb28bcba313d33","placeholder":"​","style":"IPY_MODEL_93e767b76a1c444fa353b90616fd49da","value":" 25.0/25.0 [00:00&lt;00:00, 341B/s]"}},"c0ec0f2bcca243c89a05b0bc19810c60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"689bef685b534fbaa87208e135695031":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80b1b632f7df40c1b4157369bc008aef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3795897954c47f28e7bb45211611209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ceab1e51e1a4364a6063b3cf0fc64ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a50972c0a0c54aeeb4cb28bcba313d33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93e767b76a1c444fa353b90616fd49da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c87938249ec34c06ac141946d1556a92":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fee365ecd29f419b8142f17b0d1ff0f5","IPY_MODEL_91146b2d11a644a7a096742fc0c00cd7","IPY_MODEL_8bc8aaf90e9a4edf9e50a6949741b056"],"layout":"IPY_MODEL_f1e3db639af745108c38c62689e140e2"}},"fee365ecd29f419b8142f17b0d1ff0f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa0521ca13049959b8ee1ee7b21b6ac","placeholder":"​","style":"IPY_MODEL_a5afc3fa873d45bea39aacda3f096026","value":"config.json: 100%"}},"91146b2d11a644a7a096742fc0c00cd7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2c6829c26cf4a8d8dcae6560b963c57","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11adebcc65cd43d5b8fd2326a2eded97","value":482}},"8bc8aaf90e9a4edf9e50a6949741b056":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3167c7d5df9245989aabeea29306eb95","placeholder":"​","style":"IPY_MODEL_4e5ffee377294fe6b383844dc2df8150","value":" 482/482 [00:00&lt;00:00, 7.45kB/s]"}},"f1e3db639af745108c38c62689e140e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa0521ca13049959b8ee1ee7b21b6ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5afc3fa873d45bea39aacda3f096026":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2c6829c26cf4a8d8dcae6560b963c57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11adebcc65cd43d5b8fd2326a2eded97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3167c7d5df9245989aabeea29306eb95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e5ffee377294fe6b383844dc2df8150":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8dae964280646a79e86416e10df817a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2c4d47b21394a6a880c4fdb17a103a5","IPY_MODEL_ca187d7d68c64373a37dd4bcb4c27587","IPY_MODEL_2d7ca311f3e240ee8a4cd434df0ae53f"],"layout":"IPY_MODEL_08cfffa95be34594a53e82ec1f53f017"}},"c2c4d47b21394a6a880c4fdb17a103a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a4ccb49dc834b619845a9e757bf30d6","placeholder":"​","style":"IPY_MODEL_3662834ef42449b8aef970b5bd301573","value":"vocab.json: 100%"}},"ca187d7d68c64373a37dd4bcb4c27587":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_774eff078c644a9e8b712aa78f40a7d5","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6fc9fd6e0a584f75a9af268ccd210883","value":898823}},"2d7ca311f3e240ee8a4cd434df0ae53f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe5b809fb7164578aa2b54a92f74dbb4","placeholder":"​","style":"IPY_MODEL_ff7b747592b04310b0b56356025eaf4f","value":" 899k/899k [00:00&lt;00:00, 3.98MB/s]"}},"08cfffa95be34594a53e82ec1f53f017":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a4ccb49dc834b619845a9e757bf30d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3662834ef42449b8aef970b5bd301573":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"774eff078c644a9e8b712aa78f40a7d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fc9fd6e0a584f75a9af268ccd210883":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe5b809fb7164578aa2b54a92f74dbb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff7b747592b04310b0b56356025eaf4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec8714bab72641159a4d9e4dfd4827ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c78175d7ed2b4c1f9080f96101ac5f91","IPY_MODEL_0ea30abddcc74d94921c845ee91d9b91","IPY_MODEL_40028368d8e44e928832775125016a9a"],"layout":"IPY_MODEL_b8975fb9da394547847188cd29e10443"}},"c78175d7ed2b4c1f9080f96101ac5f91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44fc1827e18949e0926c4f6c2b56fb78","placeholder":"​","style":"IPY_MODEL_34edb24fa63f42a7b0524a2e1a4662eb","value":"merges.txt: 100%"}},"0ea30abddcc74d94921c845ee91d9b91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec7bbef2cfbe4f49a0b2604cdb0effde","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d474f178704401289a9eafbbc6ecd81","value":456318}},"40028368d8e44e928832775125016a9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42816327280d4d1cb15a3d452e501099","placeholder":"​","style":"IPY_MODEL_f4b18bc09b5849c08606756daad91ad8","value":" 456k/456k [00:00&lt;00:00, 11.9MB/s]"}},"b8975fb9da394547847188cd29e10443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44fc1827e18949e0926c4f6c2b56fb78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34edb24fa63f42a7b0524a2e1a4662eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec7bbef2cfbe4f49a0b2604cdb0effde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d474f178704401289a9eafbbc6ecd81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42816327280d4d1cb15a3d452e501099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4b18bc09b5849c08606756daad91ad8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a89ca71a5940405b935b96768434326b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a8e25537333427a96814a506fd66429","IPY_MODEL_a49959f1958a494f954226a009ec519c","IPY_MODEL_a355d1083e0b4e90a30e3c314e367a5c"],"layout":"IPY_MODEL_0585db0ee8084d46b2a66db8b202e951"}},"4a8e25537333427a96814a506fd66429":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c67f8d19686d4fe59aa9782ca0dc2b2b","placeholder":"​","style":"IPY_MODEL_e289bfba147440248e500e8fefd64c0b","value":"tokenizer.json: 100%"}},"a49959f1958a494f954226a009ec519c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af7d357af54d46f1acd5eed69f5131ea","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f6e83c7efec41c5ac7ed20e55ead2b2","value":1355863}},"a355d1083e0b4e90a30e3c314e367a5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_343ee34567d4473380ec8df01767c5c5","placeholder":"​","style":"IPY_MODEL_215bc36ccd144ebdb9f548d6bc412b0f","value":" 1.36M/1.36M [00:00&lt;00:00, 1.90MB/s]"}},"0585db0ee8084d46b2a66db8b202e951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c67f8d19686d4fe59aa9782ca0dc2b2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e289bfba147440248e500e8fefd64c0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af7d357af54d46f1acd5eed69f5131ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6e83c7efec41c5ac7ed20e55ead2b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"343ee34567d4473380ec8df01767c5c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"215bc36ccd144ebdb9f548d6bc412b0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"478d9382566b44d7abdc2aad10401794":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d9123ab016d4f23a047e9a9a573e8f5","IPY_MODEL_e6d1479ea7b64b5f8c7475a67cc1a25f","IPY_MODEL_cc858a48b5044259a4e4e977565cc501"],"layout":"IPY_MODEL_9803be88c4da497f81bab8e599b43d83"}},"6d9123ab016d4f23a047e9a9a573e8f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2891c9db1e34c42a43062da29a0fbb7","placeholder":"​","style":"IPY_MODEL_f6d94700413944b8be53b17144260dd2","value":"model.safetensors: 100%"}},"e6d1479ea7b64b5f8c7475a67cc1a25f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8abe32c954f241fca4a7cb8b1622d749","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_119dbe46f3ce424a8311f59afa8f33d3","value":1421700479}},"cc858a48b5044259a4e4e977565cc501":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbbf00ddeae149c789b205b9636ece21","placeholder":"​","style":"IPY_MODEL_5edd29a42e8c48a2b045a70745fb3ccc","value":" 1.42G/1.42G [00:06&lt;00:00, 249MB/s]"}},"9803be88c4da497f81bab8e599b43d83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2891c9db1e34c42a43062da29a0fbb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6d94700413944b8be53b17144260dd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8abe32c954f241fca4a7cb8b1622d749":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"119dbe46f3ce424a8311f59afa8f33d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbbf00ddeae149c789b205b9636ece21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5edd29a42e8c48a2b045a70745fb3ccc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}