{"cells":[{"cell_type":"code","source":["! wget https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n","! pip install transformers\n","! pip install optuna\n","! pip install sentencepiece\n","! pip install gdown\n","! pip install sentence-transformers"],"metadata":{"id":"FCJ7SMS-KEW7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"efcad0a4-738d-4920-a8a7-da83330bc62b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-04-25 10:42:37--  https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3846925 (3.7M) [text/plain]\n","Saving to: ‘edos_labelled_aggregated.csv’\n","\n","edos_labelled_aggre 100%[===================>]   3.67M  --.-KB/s    in 0.07s   \n","\n","2024-04-25 10:42:37 (49.6 MB/s) - ‘edos_labelled_aggregated.csv’ saved [3846925/3846925]\n","\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Collecting optuna\n","  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna)\n","  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","Installing collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.3 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Collecting sentence-transformers\n","  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import json\n","import pickle\n","import unicodedata\n","import re\n","from tqdm import tqdm\n","from copy import deepcopy\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","import transformers\n","from transformers.optimization import get_linear_schedule_with_warmup\n","from transformers import BertModel, BertTokenizer, DebertaTokenizer, DebertaModel, RobertaTokenizer, RobertaModel, ElectraTokenizer, ElectraModel\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n","import pandas as pd\n","import os\n","from collections import defaultdict, namedtuple, OrderedDict\n","from torch.utils.data import Dataset, DataLoader\n","from itertools import count\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW, Adam, RMSprop\n","from copy import deepcopy\n","from sklearn.utils import shuffle\n","from typing import Union, Callable\n","import random\n","import gdown\n","from torch import Tensor\n","from typing import Optional, Tuple\n","import pickle as pk\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","# from sentence_transformers import SentenceTransformer"],"metadata":{"id":"zdNy2p-ZKvwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","nltk.download('punkt')"],"metadata":{"id":"ImJiVaCLMcHC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e8c4c4f1-420a-4dda-a5a9-78428771b092"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["seeds = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n","seed_idx = 1\n","seed = seeds[seed_idx]\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","from transformers import AutoTokenizer, AutoModel\n","deberta = 'microsoft/deberta-v3-large'\n","roberta = 'roberta-large'\n","model_name = deberta\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name, output_hidden_states=True)"],"metadata":{"id":"UWqn9QA9FW7K","colab":{"base_uri":"https://localhost:8080/","height":304,"referenced_widgets":["eab3b2733f0244229a8339f32f4d88cc","1830263562f34baa88cfd9bfbdee95c6","258e3ac28c5f4eba9c253da94e031e4d","e654d4d6a6fc40759a8c95a1c9e86d38","4295ef379513464db91f6fe3fd4c49c6","3057e3f168b44a3598d5e8b3bb510f0c","ade6c3d6da754fc29fdbdef3d73fa7b6","55364de8a1454880b6a44b75a3b62282","c7321926d5f74587b62362155a7828a1","d864063397a74bdf8b7ad482d6445a4a","e8d0123c06fe4d1fb8110514b9c754e9","0093a9f409ec489da122d30b321b7457","73afc8c1136a4cdd9d1dc8f1fdab65ad","e264303a08274360b19700fbc0ae2dbd","8c992cf6024844d49ed9a473fb1ca827","8f7e6d28feae4d7c8e302cff85fd38d1","82d8b66ec4974b849194042f506d0bbe","8a5582c90cae4c769e06a2625feb54b9","7207bb2780c84fb49bdb0e438c1cec07","0948b497073b4093b7d236eb757abc78","8ee60dcb9ee2480a96a2088326c6276c","c05df141bfb84f7f87be19fb003fc199","bee5f502b3be43aa898555db5ef33a16","5517fe7522a64b0fb611d3e66dbfffc2","3dd5404ddb124c749fb271be4253c8df","3596ea6a34c44857b7dd8ef71a6d3b77","2d7196ca500145d085a05740f31e1f94","beddde49adbe42e3b9abe595594b942d","12e4dcda02c643eb9ca0e2de2f15dd46","738d051aa2aa4efaaea45935ba49c4e4","5a1262ca23924d1bb98e3c3fbc2f7d00","7d0f8be7b59a4d9ea6a590a9e67a127f","b9ad9985bce4448981311ea1628ab84c","625facf4dff14717bae94deea3afbab8","d122f8df60674e2c9f698fb63825b63a","d53e3c4f9dc04068a0ff907b857581e3","42fb5fb6996d436aa18de57deb6d400e","3c781d61583640828e913b8ccdd9e96a","b5cd9caa5b174232a4580622fcc8635f","c0202848a38c4540911b1e0a8e753110","3f30c8ccbe60454aa72994414b88e448","f668526aaed74cc4a730eaf4c2b88adb","8ccc0a1e76e642fe985f0d3b999e5790","93ef4020bbfb4d2f9292359b0a3121fa"]},"outputId":"4d3d53e2-c95e-40f7-fb6e-785402abc2ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab3b2733f0244229a8339f32f4d88cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0093a9f409ec489da122d30b321b7457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee5f502b3be43aa898555db5ef33a16"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625facf4dff14717bae94deea3afbab8"}},"metadata":{}}]},{"cell_type":"code","source":["train_data = pd.read_csv('train_all_tasks.csv')\n","eval_data_A = pd.read_csv('dev_task_a_entries.csv')\n","eval_data_B = pd.read_csv('dev_task_b_entries.csv')\n","eval_data_C = pd.read_csv('dev_task_c_entries.csv')\n","\n","test_B = pd.read_csv('test_task_b_entries.csv')\n","test_C = pd.read_csv('test_task_c_entries.csv')"],"metadata":{"id":"SDUbcGB6FbZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tolist(tensor):\n","  return tensor.detach().cpu().tolist()\n","\n","def map_names_2_ids(names):\n","  A = dict()\n","  B = dict()\n","  for id, name in enumerate(names):\n","    A[name] = id\n","    B[id] = name\n","  return A, B\n","\n","def dist(x1, x2):\n","  return (x1 - x2).pow(2).sum(-1).sqrt()\n","\n","def entropy(logits):\n","  probs = F.softmax(logits, dim=-1)\n","  ent = -torch.sum((probs * torch.log2(probs)),dim=1)\n","  return ent"],"metadata":{"id":"ed5fJVHBFu5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = train_data[train_data['label_sexist'] == 'sexist'].reset_index(drop=True)\n","\n","label_category_raw = np.unique(train_data['label_category']).tolist()\n","label_category_map, category_label_map = map_names_2_ids(label_category_raw)\n","train_data['Tag_B'] = [label_category_map[i[1]] for i in train_data['label_category'].items()]\n","label_category = list(label_category_map.keys())\n","label_category = list(map(lambda x: re.sub('^\\d+\\.\\d*', '', x).strip(), label_category))\n","eval_label_B = pd.read_csv('dev_task_b_labels.csv')\n","eval_B = eval_data_B.merge(eval_label_B, on='rewire_id')\n","eval_B['Tag_B'] = [label_category_map[i[1]] for i in eval_B['label'].items()]\n","num_labels_B = len(train_data['label_category'].unique())\n","\n","label_vector_raw = np.unique(train_data['label_vector']).tolist()\n","label_vector_map, vector_label_map = map_names_2_ids(label_vector_raw)\n","train_data['Tag_C'] = [label_vector_map[i[1]] for i in train_data['label_vector'].items()]\n","label_vector = list(label_vector_map.keys())\n","label_vector = list(map(lambda x: re.sub('^\\d+\\.\\d*', '', x).strip(), label_vector))\n","eval_label_C = pd.read_csv('dev_task_c_labels.csv')\n","eval_C = eval_data_C.merge(eval_label_C, on='rewire_id')\n","eval_C['Tag_C'] = [label_vector_map[i[1]] for i in eval_C['label'].items()]\n","num_labels_C = len(train_data['label_vector'].unique())\n","\n","train_dataframe = train_data\n","eval_dataframe = eval_B\n","test_dataframe = test_B\n","new_l = label_category\n","\n","class_weights = compute_class_weight(class_weight='balanced', classes=np.array(list(range(num_labels_B))), y=train_data['Tag_B'].values.tolist()).tolist()\n"],"metadata":{"id":"2qc_VKeDF2gI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJIdNLZypaPc"},"outputs":[],"source":["class SexistDataset(Dataset):\n","  def __init__(self, dataframe, tokenizer, max_length=100, is_test=False):\n","    self.dataframe = dataframe\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","    self.labels_names = f'{tokenizer.sep_token}'.join(new_l)\n","    self.is_test = is_test\n","    self.labels_tokens = []\n","    for label_name in new_l:\n","      label_tokens = tokenizer(label_name, add_special_tokens=False)\n","      self.labels_tokens.append(label_tokens['input_ids'])\n","\n","  def __len__(self):\n","    return len(self.dataframe)\n","\n","  def __getitem__(self, idx):\n","    sample = self.dataframe.loc[idx]\n","    tokenized_text = tokenizer(\n","          sample['text'],\n","          max_length=self.max_length,\n","          padding='max_length',\n","          truncation='only_first',\n","          return_tensors='pt')\n","\n","    # find the first token of labels\n","    input_ids = tokenized_text['input_ids']\n","    labels_start = (input_ids == tokenizer.sep_token_id).nonzero().contiguous().view(-1).tolist()[1] + 2\n","\n","    labels_tokens_span = []\n","    c_token = labels_start\n","    # print(labels_start)\n","    for label_tokens in self.labels_tokens:\n","\n","      labels_tokens_span.append([c_token, c_token + len(label_tokens) - 1])\n","      c_token += len(label_tokens) + 1\n","    tokenized_text['labels_tokens_span'] = torch.tensor(labels_tokens_span)\n","    if not self.is_test:\n","      labels_B = torch.LongTensor([sample['Tag_B']])\n","      tokenized_text['Tag_B'] = labels_B\n","    return tokenized_text\n","\n","\n","class PGD():\n","    def __init__(self, model,emb_name,epsilon=1.,alpha=0.3):\n","        # The emb_name parameter should be replaced with the parameter name of the embedding in your model\n","        self.model = model\n","        self.emb_name = emb_name\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.emb_backup = {}\n","        self.grad_backup = {}\n","\n","    # adversarial training : attack to change embedding abit with regards projected gradiant descent\n","    def attack(self,first_strike=False):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and self.emb_name in name:\n","                if first_strike:\n","                    # print('tt', param.data)\n","                    self.emb_backup[name] = param.data.clone()\n","                norm = torch.norm(param.grad)\n","                if norm != 0:\n","                    # Compute new params\n","                    r_at = self.alpha * param.grad / norm\n","                    param.data.add_(r_at)\n","                    param.data = self.project(name, param.data, self.epsilon)\n","\n","    # Restore to the back-up embeddings\n","    def restore(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and self.emb_name in name:\n","                assert name in self.emb_backup\n","                param.data = self.emb_backup[name]\n","        self.emb_backup = {}\n","\n","    # Project Gradiant Descent\n","    def project(self, param_name, param_data, epsilon):\n","        r = param_data - self.emb_backup[param_name]\n","        if torch.norm(r) > epsilon:\n","            r = epsilon * r / torch.norm(r)\n","        return self.emb_backup[param_name] + r\n","\n","    # Back-up parameters\n","    def backup_grad(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and 'pooler' not in name:\n","                self.grad_backup[name] = param.grad.clone()\n","\n","    # Restore grad parameters\n","    def restore_grad(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and 'pooler' not in name:\n","                param.grad = self.grad_backup[name]\n","\n","\n","class SexistModel(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.transformer = model\n","        hidden_size = self.transformer.config.hidden_size\n","        self.dropout = nn.Dropout(p=.3)\n","        self.head = nn.Linear(hidden_size, num_labels_B)\n","\n","    def integrate(self, batch_output, batch_labels_tokens_span):\n","      batch_size = batch_output.shape[0]\n","      integrated_batch = []\n","      for i in range(batch_size):\n","        integrated_sample_labels = []\n","        output = batch_output[i]\n","        labels_tokens_span = batch_labels_tokens_span[i]\n","        for label_tokens_span in labels_tokens_span:\n","          integrated_label = output[label_tokens_span[0].item(): label_tokens_span[1].item() + 1].mean(0).view(-1)\n","          assert integrated_label.shape[0] == self.transformer.config.hidden_size\n","          integrated_sample_labels.append(integrated_label)\n","        integrated_sample_labels = torch.stack(integrated_sample_labels)\n","        integrated_batch.append(integrated_sample_labels)\n","      integrated_batch = torch.stack(integrated_batch)\n","      return integrated_batch\n","\n","    def forward(self, x, batch_labels_tokens_span, vat=False, attention_mask=None):\n","        if vat:\n","          hidden = self.transformer(inputs_embeds=x, attention_mask=attention_mask).last_hidden_state\n","        else:\n","          hidden = self.transformer(**x).last_hidden_state\n","        cls = hidden[:, 0, :]\n","        x = self.head(cls)\n","        x = x.view(-1, num_labels_B)\n","        return x, hidden"]},{"cell_type":"code","source":["def train(dataloader, model, device, loss_fn, optimizer, scheduler, stage, ul_dataset, use_contrastive=False,\n","          use_adv=True, use_vadv=False, use_ul=False, vat_weight=.5, ul_weight=.5, con_weight=.5, adv_use_every_layer=True):\n","\n","  model.train()\n","  named_weights = [n for n, _ in model.named_parameters() if 'dense.weight' in n and 'pooler' not in n] + [\"word_embeddings.\"]\n","  loss_collection = [[], [], [], [], []]\n","  for step, data in enumerate(dataloader):\n","\n","    if adv_use_every_layer:\n","      rand_layer = random.sample(named_weights, 1)[0]\n","      adv_layer = rand_layer\n","    else:\n","      adv_layer = \"word_embeddings.\"\n","    pgd = PGD(\n","      model=model,\n","      emb_name=adv_layer\n","    )\n","\n","    c_batch_size = data['input_ids'].shape[0]\n","    labels = data.pop('Tag_B').to(device).view(-1)\n","    for key in data:\n","      data[key] = data[key].to(device).view(c_batch_size, -1)\n","    batch_labels_tokens_span = data.pop('labels_tokens_span').view(-1, num_labels_B, 2)\n","\n","    logits, _ = model(data, batch_labels_tokens_span)\n","\n","    ce_loss = loss_fn(logits, labels)\n","    ce_loss.backward()\n","    loss_collection[0].append(ce_loss.item())\n","\n","\n","    if use_adv:\n","      # PGD Start\n","        pgd.backup_grad()\n","        attack_times = 2\n","        for attack_time in range(attack_times):\n","            # Add adversarial perturbation to the embedding, backup param.data during the first attack\n","            pgd.attack(first_strike=(attack_time==0))\n","            if attack_time != attack_times-1:\n","              model.zero_grad()\n","            else:\n","              pgd.restore_grad()\n","            logits_adv, _ = model(data, batch_labels_tokens_span)\n","            loss_adv = loss_fn(logits_adv, labels)\n","            loss_collection[1].append(loss_adv.item())\n","            loss_adv.backward()\n","        # Restore embedding parameters\n","        pgd.restore()\n","\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    scheduler.step()\n","\n","    if len(loss_collection[0]) % log_step == 0:\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | CE Loss {round(sum(loss_collection[0]) / (len(loss_collection[0]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | ADV Loss {round(sum(loss_collection[1]) / (len(loss_collection[1]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | CON Loss {round(sum(loss_collection[2]) / (len(loss_collection[2]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | VAT Loss {round(sum(loss_collection[3]) / (len(loss_collection[3]) + 1e-8), 4)}')\n","      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | UL Loss {round(sum(loss_collection[4]) / (len(loss_collection[4]) + 1e-8), 4)}')\n","      print('------------------------------------------------')\n","      loss_collection = [[] for _ in range(5)]"],"metadata":{"id":"8CF-CossGPmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(dataloader, model, device):\n","  with torch.no_grad():\n","    model.eval()\n","    all_preds = list()\n","\n","    for data in dataloader:\n","      c_batch_size = data['input_ids'].shape[0]\n","      for key in data:\n","        data[key] = data[key].to(device).view(c_batch_size, -1)\n","      batch_labels_tokens_span = data.pop('labels_tokens_span').view(-1, num_labels_B, 2)\n","      Tag_B = data.pop('Tag_B').to(device).view(-1)\n","      logits, _ = model(data, batch_labels_tokens_span)\n","      preds = tolist(logits.argmax(1).view(-1))\n","      all_preds.extend(preds)\n","  return all_preds\n","\n","\n","def test(dataloader, model, device):\n","  with torch.no_grad():\n","    model.eval()\n","    all_preds = list()\n","\n","    for data in dataloader:\n","      c_batch_size = data['input_ids'].shape[0]\n","      for key in data:\n","        data[key] = data[key].to(device).view(c_batch_size, -1)\n","      batch_labels_tokens_span = data.pop('labels_tokens_span').view(-1, num_labels_B, 2)\n","      logits, _ = model(data, batch_labels_tokens_span)\n","      preds = tolist(logits.argmax(1).view(-1))\n","      all_preds.extend(preds)\n","  return all_preds"],"metadata":{"id":"wLV-6T3oNF6q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 8\n","lr = 1e-5\n","beta_1 = .9\n","beta_2 = .999\n","eps = 1e-6\n","log_step = 100\n","batch_size = 10\n","weight_decay = 9e-3\n","max_length = 70\n","loss_file = 'loss.txt'\n","eval_file = 'eval.txt'\n","\n","vat_weight = .5\n","ul_weight = .5\n","ent_weight = .5\n","\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","# sexist_model = ExtractedRoBERTa(deepcopy(model)).to(device)\n","sexist_model = SexistModel(deepcopy(model)).to(device)\n","loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device)).to(device)\n","loss_collection = []\n","\n","train_dataset = SexistDataset(train_dataframe, tokenizer, max_length)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","eval_dataset = SexistDataset(eval_dataframe, tokenizer, max_length)\n","eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n","\n","test_dataset = SexistDataset(test_dataframe, tokenizer, max_length, is_test=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","opt_step = 0\n","optimization_steps = epochs * len(train_dataloader)\n","warmup_ratio = .0\n","warmup_steps = int(optimization_steps * warmup_ratio)\n","\n","\n","optimizer = AdamW(sexist_model.parameters(), lr=lr, betas=(beta_1,beta_2), eps=eps, weight_decay=weight_decay)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=warmup_steps,\n","    num_training_steps=optimization_steps)\n","\n","best_f1 = 0.\n","best_model = None\n","transformers.logging.set_verbosity_error()"],"metadata":{"id":"LaURbAcVNN7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = 'Task_B/'\n","filename = os.path.join(checkpoint_dir, 'best_ch.pt')\n","\n","\n","try:\n","    os.rmdir(checkpoint_dir)\n","except:\n","    pass\n","if not os.path.exists(checkpoint_dir):\n","  os.mkdir(checkpoint_dir)\n","\n","def save_model(epoch, model, optimizer, scheduler):\n","  filename = os.path.join(checkpoint_dir, 'best_ch.pt')\n","  torch.save(\n","      {'epoch': epoch,\n","       'model_state_dict': model.state_dict(),\n","       'optimizer_state_dict': optimizer.state_dict(),\n","       'scheduler_state_dict': scheduler.state_dict()},\n","        filename)\n","\n","def load_model():\n","  if os.path.exists(filename):\n","    saved_dict = torch.load(filename)\n","    return True, saved_dict\n","  else:\n","    return False, None\n","\n","\n","def early_stop(scores, current_score, patience, best_f1):\n","  if len(scores) < patience:\n","    return False\n","  else:\n","    for score in scores[-patience: ]:\n","      if score >= best_f1:\n","        return False\n","    return True"],"metadata":{"id":"AgXyfS3TNWaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_f1 = list()\n","patience = 4\n","\n","for epoch in range(epochs):\n","  train(train_dataloader, sexist_model, device, loss_fn, optimizer, scheduler, 1, None)\n","  preds_B_eval = eval(eval_dataloader, sexist_model, device)\n","  f1_macro_B_eval = f1_score(eval_dataframe['Tag_B'].values.tolist(), preds_B_eval, average='macro')\n","  all_f1.append(f1_macro_B_eval)\n","  if f1_macro_B_eval > best_f1:\n","    best_f1 = f1_macro_B_eval\n","    best_preds = preds_B_eval\n","    save_model(epoch + 1, sexist_model, optimizer, scheduler)\n","\n","  print(f'EPOCH [{epoch + 1}/{epochs}] | Current F1-Macro {round(f1_macro_B_eval * 100, 2)}')\n","  print(f'EPOCH [{epoch + 1}/{epochs}] | Best F1-Macro {round(best_f1 * 100, 2)}')\n","  print(confusion_matrix(eval_dataframe['Tag_B'].values.tolist(), preds_B_eval))\n","\n","  if early_stop(all_f1, f1_macro_B_eval, patience, best_f1):\n","    break\n","  else:\n","    print('not early stopping')"],"metadata":{"id":"jU-GAk2LGUD3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"695aa575-ab35-4cc6-9420-9c8885370c9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH [1/8] | STEP [100/340] | CE Loss 1.3905\n","EPOCH [1/8] | STEP [100/340] | ADV Loss 1.4376\n","EPOCH [1/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [1/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [1/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/8] | STEP [200/340] | CE Loss 1.3757\n","EPOCH [1/8] | STEP [200/340] | ADV Loss 1.4273\n","EPOCH [1/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [1/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [1/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/8] | STEP [300/340] | CE Loss 1.325\n","EPOCH [1/8] | STEP [300/340] | ADV Loss 1.3916\n","EPOCH [1/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [1/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [1/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [1/8] | Current F1-Macro 49.66\n","EPOCH [1/8] | Best F1-Macro 49.66\n","[[ 33   7   1   3]\n"," [ 13 196  13   5]\n"," [  7 144  11   5]\n"," [  2  21   2  23]]\n","not early stopping\n","EPOCH [2/8] | STEP [100/340] | CE Loss 0.9143\n","EPOCH [2/8] | STEP [100/340] | ADV Loss 1.1429\n","EPOCH [2/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [2/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [2/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/8] | STEP [200/340] | CE Loss 0.8481\n","EPOCH [2/8] | STEP [200/340] | ADV Loss 1.0626\n","EPOCH [2/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [2/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [2/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/8] | STEP [300/340] | CE Loss 0.8743\n","EPOCH [2/8] | STEP [300/340] | ADV Loss 1.0862\n","EPOCH [2/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [2/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [2/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [2/8] | Current F1-Macro 54.68\n","EPOCH [2/8] | Best F1-Macro 54.68\n","[[ 38   0   1   5]\n"," [ 24 106  44  53]\n"," [ 11  37  79  40]\n"," [  4   0   2  42]]\n","not early stopping\n","EPOCH [3/8] | STEP [100/340] | CE Loss 0.559\n","EPOCH [3/8] | STEP [100/340] | ADV Loss 0.7567\n","EPOCH [3/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [3/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [3/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/8] | STEP [200/340] | CE Loss 0.5306\n","EPOCH [3/8] | STEP [200/340] | ADV Loss 0.7563\n","EPOCH [3/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [3/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [3/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/8] | STEP [300/340] | CE Loss 0.5415\n","EPOCH [3/8] | STEP [300/340] | ADV Loss 0.7647\n","EPOCH [3/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [3/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [3/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [3/8] | Current F1-Macro 67.72\n","EPOCH [3/8] | Best F1-Macro 67.72\n","[[ 38   2   2   2]\n"," [ 14 121  86   6]\n"," [  6  10 138  13]\n"," [  3   6   7  32]]\n","not early stopping\n","EPOCH [4/8] | STEP [100/340] | CE Loss 0.3838\n","EPOCH [4/8] | STEP [100/340] | ADV Loss 0.5578\n","EPOCH [4/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [4/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [4/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [4/8] | STEP [200/340] | CE Loss 0.3332\n","EPOCH [4/8] | STEP [200/340] | ADV Loss 0.5261\n","EPOCH [4/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [4/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [4/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [4/8] | STEP [300/340] | CE Loss 0.3843\n","EPOCH [4/8] | STEP [300/340] | ADV Loss 0.5814\n","EPOCH [4/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [4/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [4/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [4/8] | Current F1-Macro 69.43\n","EPOCH [4/8] | Best F1-Macro 69.43\n","[[ 38   3   1   2]\n"," [ 10 165  44   8]\n"," [  8  38 108  13]\n"," [  2   8   6  32]]\n","not early stopping\n","EPOCH [5/8] | STEP [100/340] | CE Loss 0.2628\n","EPOCH [5/8] | STEP [100/340] | ADV Loss 0.4259\n","EPOCH [5/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [5/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [5/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [5/8] | STEP [200/340] | CE Loss 0.2564\n","EPOCH [5/8] | STEP [200/340] | ADV Loss 0.4217\n","EPOCH [5/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [5/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [5/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [5/8] | STEP [300/340] | CE Loss 0.2343\n","EPOCH [5/8] | STEP [300/340] | ADV Loss 0.3879\n","EPOCH [5/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [5/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [5/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [5/8] | Current F1-Macro 71.94\n","EPOCH [5/8] | Best F1-Macro 71.94\n","[[ 39   2   1   2]\n"," [ 10 177  35   5]\n"," [  5  45 111   6]\n"," [  3   8   8  29]]\n","not early stopping\n","EPOCH [6/8] | STEP [100/340] | CE Loss 0.1877\n","EPOCH [6/8] | STEP [100/340] | ADV Loss 0.3312\n","EPOCH [6/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [6/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [6/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [6/8] | STEP [200/340] | CE Loss 0.1481\n","EPOCH [6/8] | STEP [200/340] | ADV Loss 0.3085\n","EPOCH [6/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [6/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [6/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [6/8] | STEP [300/340] | CE Loss 0.1963\n","EPOCH [6/8] | STEP [300/340] | ADV Loss 0.3153\n","EPOCH [6/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [6/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [6/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [6/8] | Current F1-Macro 71.18\n","EPOCH [6/8] | Best F1-Macro 71.94\n","[[ 38   3   1   2]\n"," [  9 179  33   6]\n"," [  3  48 110   6]\n"," [  3  11   7  27]]\n","not early stopping\n","EPOCH [7/8] | STEP [100/340] | CE Loss 0.1203\n","EPOCH [7/8] | STEP [100/340] | ADV Loss 0.2522\n","EPOCH [7/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [7/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [7/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [7/8] | STEP [200/340] | CE Loss 0.1242\n","EPOCH [7/8] | STEP [200/340] | ADV Loss 0.2793\n","EPOCH [7/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [7/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [7/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [7/8] | STEP [300/340] | CE Loss 0.1175\n","EPOCH [7/8] | STEP [300/340] | ADV Loss 0.2362\n","EPOCH [7/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [7/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [7/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [7/8] | Current F1-Macro 72.14\n","EPOCH [7/8] | Best F1-Macro 72.14\n","[[ 38   3   1   2]\n"," [  8 169  45   5]\n"," [  1  40 120   6]\n"," [  2  11   8  27]]\n","not early stopping\n","EPOCH [8/8] | STEP [100/340] | CE Loss 0.0946\n","EPOCH [8/8] | STEP [100/340] | ADV Loss 0.2223\n","EPOCH [8/8] | STEP [100/340] | CON Loss 0.0\n","EPOCH [8/8] | STEP [100/340] | VAT Loss 0.0\n","EPOCH [8/8] | STEP [100/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [8/8] | STEP [200/340] | CE Loss 0.1004\n","EPOCH [8/8] | STEP [200/340] | ADV Loss 0.2144\n","EPOCH [8/8] | STEP [200/340] | CON Loss 0.0\n","EPOCH [8/8] | STEP [200/340] | VAT Loss 0.0\n","EPOCH [8/8] | STEP [200/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [8/8] | STEP [300/340] | CE Loss 0.0925\n","EPOCH [8/8] | STEP [300/340] | ADV Loss 0.2177\n","EPOCH [8/8] | STEP [300/340] | CON Loss 0.0\n","EPOCH [8/8] | STEP [300/340] | VAT Loss 0.0\n","EPOCH [8/8] | STEP [300/340] | UL Loss 0.0\n","------------------------------------------------\n","EPOCH [8/8] | Current F1-Macro 72.1\n","EPOCH [8/8] | Best F1-Macro 72.14\n","[[ 37   4   1   2]\n"," [  7 178  36   6]\n"," [  1  49 111   6]\n"," [  2  11   7  28]]\n","not early stopping\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CJ5oriTGMk1c"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"eab3b2733f0244229a8339f32f4d88cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1830263562f34baa88cfd9bfbdee95c6","IPY_MODEL_258e3ac28c5f4eba9c253da94e031e4d","IPY_MODEL_e654d4d6a6fc40759a8c95a1c9e86d38"],"layout":"IPY_MODEL_4295ef379513464db91f6fe3fd4c49c6"}},"1830263562f34baa88cfd9bfbdee95c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3057e3f168b44a3598d5e8b3bb510f0c","placeholder":"​","style":"IPY_MODEL_ade6c3d6da754fc29fdbdef3d73fa7b6","value":"tokenizer_config.json: 100%"}},"258e3ac28c5f4eba9c253da94e031e4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55364de8a1454880b6a44b75a3b62282","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7321926d5f74587b62362155a7828a1","value":52}},"e654d4d6a6fc40759a8c95a1c9e86d38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d864063397a74bdf8b7ad482d6445a4a","placeholder":"​","style":"IPY_MODEL_e8d0123c06fe4d1fb8110514b9c754e9","value":" 52.0/52.0 [00:00&lt;00:00, 1.08kB/s]"}},"4295ef379513464db91f6fe3fd4c49c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3057e3f168b44a3598d5e8b3bb510f0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ade6c3d6da754fc29fdbdef3d73fa7b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55364de8a1454880b6a44b75a3b62282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7321926d5f74587b62362155a7828a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d864063397a74bdf8b7ad482d6445a4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8d0123c06fe4d1fb8110514b9c754e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0093a9f409ec489da122d30b321b7457":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73afc8c1136a4cdd9d1dc8f1fdab65ad","IPY_MODEL_e264303a08274360b19700fbc0ae2dbd","IPY_MODEL_8c992cf6024844d49ed9a473fb1ca827"],"layout":"IPY_MODEL_8f7e6d28feae4d7c8e302cff85fd38d1"}},"73afc8c1136a4cdd9d1dc8f1fdab65ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82d8b66ec4974b849194042f506d0bbe","placeholder":"​","style":"IPY_MODEL_8a5582c90cae4c769e06a2625feb54b9","value":"config.json: 100%"}},"e264303a08274360b19700fbc0ae2dbd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7207bb2780c84fb49bdb0e438c1cec07","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0948b497073b4093b7d236eb757abc78","value":580}},"8c992cf6024844d49ed9a473fb1ca827":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ee60dcb9ee2480a96a2088326c6276c","placeholder":"​","style":"IPY_MODEL_c05df141bfb84f7f87be19fb003fc199","value":" 580/580 [00:00&lt;00:00, 22.6kB/s]"}},"8f7e6d28feae4d7c8e302cff85fd38d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82d8b66ec4974b849194042f506d0bbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a5582c90cae4c769e06a2625feb54b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7207bb2780c84fb49bdb0e438c1cec07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0948b497073b4093b7d236eb757abc78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ee60dcb9ee2480a96a2088326c6276c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c05df141bfb84f7f87be19fb003fc199":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bee5f502b3be43aa898555db5ef33a16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5517fe7522a64b0fb611d3e66dbfffc2","IPY_MODEL_3dd5404ddb124c749fb271be4253c8df","IPY_MODEL_3596ea6a34c44857b7dd8ef71a6d3b77"],"layout":"IPY_MODEL_2d7196ca500145d085a05740f31e1f94"}},"5517fe7522a64b0fb611d3e66dbfffc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_beddde49adbe42e3b9abe595594b942d","placeholder":"​","style":"IPY_MODEL_12e4dcda02c643eb9ca0e2de2f15dd46","value":"spm.model: 100%"}},"3dd5404ddb124c749fb271be4253c8df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_738d051aa2aa4efaaea45935ba49c4e4","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a1262ca23924d1bb98e3c3fbc2f7d00","value":2464616}},"3596ea6a34c44857b7dd8ef71a6d3b77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d0f8be7b59a4d9ea6a590a9e67a127f","placeholder":"​","style":"IPY_MODEL_b9ad9985bce4448981311ea1628ab84c","value":" 2.46M/2.46M [00:00&lt;00:00, 21.9MB/s]"}},"2d7196ca500145d085a05740f31e1f94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"beddde49adbe42e3b9abe595594b942d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12e4dcda02c643eb9ca0e2de2f15dd46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"738d051aa2aa4efaaea45935ba49c4e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a1262ca23924d1bb98e3c3fbc2f7d00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d0f8be7b59a4d9ea6a590a9e67a127f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9ad9985bce4448981311ea1628ab84c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"625facf4dff14717bae94deea3afbab8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d122f8df60674e2c9f698fb63825b63a","IPY_MODEL_d53e3c4f9dc04068a0ff907b857581e3","IPY_MODEL_42fb5fb6996d436aa18de57deb6d400e"],"layout":"IPY_MODEL_3c781d61583640828e913b8ccdd9e96a"}},"d122f8df60674e2c9f698fb63825b63a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5cd9caa5b174232a4580622fcc8635f","placeholder":"​","style":"IPY_MODEL_c0202848a38c4540911b1e0a8e753110","value":"pytorch_model.bin: 100%"}},"d53e3c4f9dc04068a0ff907b857581e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f30c8ccbe60454aa72994414b88e448","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f668526aaed74cc4a730eaf4c2b88adb","value":873673253}},"42fb5fb6996d436aa18de57deb6d400e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ccc0a1e76e642fe985f0d3b999e5790","placeholder":"​","style":"IPY_MODEL_93ef4020bbfb4d2f9292359b0a3121fa","value":" 874M/874M [00:06&lt;00:00, 125MB/s]"}},"3c781d61583640828e913b8ccdd9e96a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5cd9caa5b174232a4580622fcc8635f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0202848a38c4540911b1e0a8e753110":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f30c8ccbe60454aa72994414b88e448":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f668526aaed74cc4a730eaf4c2b88adb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ccc0a1e76e642fe985f0d3b999e5790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93ef4020bbfb4d2f9292359b0a3121fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}